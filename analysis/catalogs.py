# -*- coding: utf-8 -*-
"""catalogs.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CEqUZyxPbLq-CgwS1y-jDlWpMjZrtE28

#Import Statements
"""

import pandas as pd
import numpy as np
import nltk
import spacy
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')
import string
import re
from re import *
from nltk.util import ngrams
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer

# mount the drive module for importing data from Google Drive
#from google.colab import drive
#drive.mount('/content/drive')

# path to data directory on my drive
#data_path = '/content/drive/My Drive/TRIPODS-Student Accessible Files/Sum2020/Difficulty Protocol/Difficulty Protocol Ellie and Sydney/'

"""#Function Definitions

##Rename Columns Function
IN:
- csv: csv name
- og_name: original column name
- new_name: new column name
"""

'''
IN: csv name, original column name, new column name
'''

def rename_col(csv, og_name, new_name):
    csv.rename(columns={og_name:new_name},inplace=True)

"""##Word Tokenizer
IN:
- col: column of responses

OUT:
- list_of_lists: list of lists tokenized by word without any preprocessing
"""

def tokenize(col):
    #creates list of lists, inside lists contains sentences tokenized by word
    list_of_lists = []
    for sentence in col:
      tokens = nltk.word_tokenize(str(sentence))
      list_of_lists.append(tokens)
    return list_of_lists

"""##Retrieve Dataframe Name Function
IN:
- df: dataframe

OUT:
- name: name of dataframe
"""

def get_df_name(df):
    name =[x for x in globals() if globals()[x] is df][0]
    return name

"""##Process Responses Across All Columns Function
IN:
- stops: list of stop words
- df: dataframe of all courses

No OUT, creates a dictionary of dictionaries containing processed responses sorted by each school for usage in TF IDF analysis
"""

def process_words(stops,df):
  og_dict = {} #dictionary of full responses
  responses = {} #dictionary of processed responses
  school = df['School'][0]
  og_dict[school] = []
  i=0
  for s in df['School']:
    if s == school:
      r = df['Descriptions'][i] #description string
      og_dict[school].append(r)
      i+=1
    else:
      school = s
      og_dict[school] = []
      r = df['Descriptions'][i] #description string
      og_dict[school].append(r)
      i+=1

  for s in list(og_dict.keys()):
    temp_list = tokenize(og_dict[s])
    responses[s] = []
    for lst in temp_list:
      lst = ' '.join(lst)
      wr_string = lst.lower()
      doc = nlp(wr_string)
      new_sent = " ".join([token.lemma_ for token in doc])
      new_sent = new_sent.split()
      new_sent = [word for word in new_sent if word not in stops]
      new_sent = ' '.join(new_sent) #completely processed sentence as string
      responses[s].append(new_sent)
  return responses, list(og_dict.keys()) #returns processeed response dictionary & school names

"""##TF-IDF Function
IN:
- dictionary: dictionary being accessed
- schools: list of school names

No OUT, creates csv file for each category in categories
"""

def tfidf(dictionary, schools):
  nlp = spacy.load("en")
  answers = dictionary
  corpus = []
  #for school in schools:
    #doc_string = ' '.join(dictionary[school])
    #corpus.append(doc_string)
  for school in schools:
    print("School: %s -----------------" % school)
    corpus=dictionary[school]
    #code from https://www.geeksforgeeks.org/tf-idf-for-bigrams-trigrams/
    #GETTING BIGRAMS
    vectorizer = CountVectorizer(ngram_range = (1,1)) 
    X1 = vectorizer.fit_transform(corpus)  
    features = (vectorizer.get_feature_names()) 
   
    # Applying TFIDF 
    vectorizer = TfidfVectorizer() 
    vectors = vectorizer.fit_transform(corpus) 
    scores = (vectors.toarray()) 
    # Getting top ranking features 
    sums = vectors.sum(axis = 0) 
    data1 = [] 
    for col, term in enumerate(features): 
        data1.append( (term, sums[0,col] )) 
    ranking = pd.DataFrame(data1, columns = ['term','rank']) 
    words = (ranking.sort_values('rank', ascending = False)) 
    words = words[:10]

    #Getting frequency counts
    freq_list = []
    list_bigram = list(words['term'])
    i = 0
    while i < 10:
      bigram = str(list_bigram[i]).split()
      f_word = str(bigram[0])
      #s_word = str(bigram[1])
      regex = re.escape(f_word) #+ r" " + re.escape(s_word)
      count = 0 
      for doc in corpus:
        iterator = finditer(regex, doc)
        for match in iterator:
          count+=1
      app = str(count)
      freq_list.append(app)
      i+=1
    words['frequency'] = freq_list
    denom = len(dictionary[school])
    words ['overall'] = str(denom)
    words['category'] = school
    words = words[['category','term','frequency','overall','rank']]
    print('\n',words)
    print("\n---------------------------\n")
'''
    #saving dataframe to csv
    path_name = 'TFIDF_' + school +'.csv'
    words.to_csv(path_name)
'''

"""##CSV Combine Function
IN:
- new_csv: name of new csv file
- old_csvs: list of old csvs

No OUT, creates one csv file of merged data from old_csvs
"""

def combine_csv(new_csv,old_csvs):
  all_files = old_csvs
  df_merged = pd.concat(all_files, ignore_index=True)
  df_merged.to_csv(new_csv,index=False)

"""#Main

##Import CSVs as Dataframes
"""

#load datasets
#schools_list = []
#alma = pd.read_csv("/content/AlmaSUPERTRIMMED.csv", index_col = 0)
#schools_list.append(alma)
#brown = pd.read_csv("/content/BrownSUPERTRIMMED.csv", index_col = 0)
#schools_list.append(brown)
#combine_csv("/content/CombinedSUPERTRIMMED.csv",schools_list)
schools = pd.read_csv("/content/AllSchools.csv")

"""##Lemmatize and Tokenize
Groups for Stop Words:
* Verbs that are not specific to data science
* Adjectives
* Adverbs
"""

#stop words definition
stop_words = list(stopwords.words('english'))
nlp = spacy.load('en', disable=['parser', 'ner'])
stop_words.append('-PRON-')
#stop_words.extend(list(set(string.punctuation)))
#extra_verbs = []
#extra_other = ['-PRON-','cr','ul','registration']
#stop_words.extend(extra_verbs)
#stop_words.extend(extra_other)

#process words, create dictionaries for future function calls
responses, school_list = process_words(stop_words,schools)

"""##TF-IDF Calculations"""

tfidf(responses,school_list)

"""##CURRENTLY UNUSED Create Two Overall .csv files (one for each analysis)"""

#create combined csv for roles
old = []
old.append(pd.read_csv(data_path+"stu.csv"))
old.append(pd.read_csv(data_path+"ta.csv"))
old.append(pd.read_csv(data_path+"prof.csv"))
combine_csv("all_roles.csv",old)
#create combined csv for questions
old = []
old.append(pd.read_csv(data_path+"topics.csv"))
old.append(pd.read_csv(data_path+"struggles.csv"))
old.append(pd.read_csv(data_path+"questions.csv"))
old.append(pd.read_csv(data_path+"surprises.csv"))
combine_csv("all_questions.csv",old)